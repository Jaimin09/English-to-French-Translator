{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from my_nmt_utils import *\n",
    "from nmt_utils import *\n",
    "\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Activation, RepeatVector, Lambda, Concatenate, Permute, Dot, Input, Multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in vacab: 382\n",
      "total words in vacab: 323\n"
     ]
    }
   ],
   "source": [
    "french_vocab = get_vocab('french_set.txt')\n",
    "french_vocab = {word:i for i, word in enumerate(french_vocab)}\n",
    "inv_french_vocab = {i:word for i, word in enumerate(french_vocab)}\n",
    "\n",
    "english_vocab = get_vocab('english_set.txt')\n",
    "english_vocab = {word:i for i, word in enumerate(english_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_data = np.genfromtxt('french_set.txt', dtype = 'str', delimiter = ',', encoding = 'utf8')\n",
    "english_data = np.genfromtxt('english_set.txt', dtype = 'str', delimiter = ',', encoding = 'utf8')\n",
    "\n",
    "max_len_fr = get_max_length(french_data)\n",
    "max_len_en = get_max_length(english_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr.shape: (200, 12)\n",
      "en.shape: (200, 11)\n",
      "fr_oh.shape: (200, 12, 382)\n",
      "en_oh.shape: (200, 11, 323)\n"
     ]
    }
   ],
   "source": [
    "Ten = max_len_en\n",
    "Tfr = max_len_fr\n",
    "fr, fr_oh = convert_string_data_to_onehot(french_data, french_vocab, Tfr)\n",
    "en, en_oh = convert_string_data_to_onehot(english_data, english_vocab, Ten)\n",
    "print(\"fr.shape:\", fr.shape)\n",
    "print(\"en.shape:\", en.shape)\n",
    "print(\"fr_oh.shape:\", fr_oh.shape)\n",
    "print(\"en_oh.shape:\", en_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator = RepeatVector(Ten)\n",
    "concatenator = Concatenate(axis = -1)\n",
    "densor = Dense(1, activation = 'relu')\n",
    "activator = Activation('softmax')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attension(a, s_prev):\n",
    "    # a = hidden state of Bi-LSTM of shape (m, Tx, 2*n_a)\n",
    "    # s_prev = previous hidden state of post attension LSTM layer (creating as post_att.. = LSTM) of shape (m, n_s)\n",
    "    \n",
    "    s_prev = repeator(s_prev)           # so that its shape becomes (m, Tx, n_s) to concatenate with a\n",
    "    \n",
    "    concat = concatenator([a, s_prev])\n",
    "    \n",
    "    energy = densor(concat)\n",
    "    \n",
    "    alpha = activator(energy)\n",
    "    \n",
    "    context = dotor([alpha, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 2*n_a\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(french_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, english_vocab_size, french_vocab_size, max_len_fr):\n",
    "    \n",
    "    X = Input(shape=(Tx, english_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n",
    "    \n",
    "    counter = 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        context = one_step_attension(a, s)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])       # s= hidden state, c= cell state\n",
    "        \n",
    "        out = output_layer(s)\n",
    "        \n",
    "        outputs.append(out)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if counter == max_len_fr:\n",
    "            stop = True\n",
    "        \n",
    "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_model = model(Ten, Tfr, n_a, n_s, len(english_vocab), len(french_vocab), max_len_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nm_model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = english_data.shape[0]\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(fr_oh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200/200 [==============================] - ETA: 2s - loss: 15.1838 - dense_2_loss_1: 2.7690 - dense_2_loss_2: 3.2089 - dense_2_loss_3: 3.5787 - dense_2_loss_4: 2.2973 - dense_2_loss_5: 1.0891 - dense_2_loss_6: 1.0867 - dense_2_loss_7: 0.6397 - dense_2_loss_8: 0.2224 - dense_2_loss_9: 0.2652 - dense_2_loss_10: 0.0143 - dense_2_loss_11: 0.0071 - dense_2_loss_12: 0.0053 - dense_2_acc_1: 0.2500 - dense_2_acc_2: 0.3125 - dense_2_acc_3: 0.3125 - dense_2_acc_4: 0.4375 - dense_2_acc_5: 0.6875 - dense_2_acc_6: 0.7500 - dense_2_acc_7: 0.8750 - dense_2_acc_8: 0.9375 - dense_2_acc_9: 0.9375 - dense_2_acc_10: 1.0000 - dense_2_acc_11: 1.0000 - dense_2_acc_12: 1.00 - ETA: 2s - loss: 14.9476 - dense_2_loss_1: 2.7947 - dense_2_loss_2: 3.3270 - dense_2_loss_3: 3.3466 - dense_2_loss_4: 2.5023 - dense_2_loss_5: 1.1072 - dense_2_loss_6: 1.1424 - dense_2_loss_7: 0.3977 - dense_2_loss_8: 0.1653 - dense_2_loss_9: 0.1415 - dense_2_loss_10: 0.0114 - dense_2_loss_11: 0.0064 - dense_2_loss_12: 0.0051 - dense_2_acc_1: 0.2812 - dense_2_acc_2: 0.3125 - dense_2_acc_3: 0.2812 - dense_2_acc_4: 0.3750 - dense_2_acc_5: 0.7500 - dense_2_acc_6: 0.7188 - dense_2_acc_7: 0.9375 - dense_2_acc_8: 0.9375 - dense_2_acc_9: 0.9688 - dense_2_acc_10: 1.0000 - dense_2_acc_11: 1.0000 - dense_2_acc_12: 1.00 - ETA: 1s - loss: 15.3530 - dense_2_loss_1: 2.7261 - dense_2_loss_2: 3.1953 - dense_2_loss_3: 3.3844 - dense_2_loss_4: 2.4435 - dense_2_loss_5: 1.3853 - dense_2_loss_6: 1.1251 - dense_2_loss_7: 0.5306 - dense_2_loss_8: 0.1938 - dense_2_loss_9: 0.1607 - dense_2_loss_10: 0.0985 - dense_2_loss_11: 0.0971 - dense_2_loss_12: 0.0126 - dense_2_acc_1: 0.3125 - dense_2_acc_2: 0.3542 - dense_2_acc_3: 0.2500 - dense_2_acc_4: 0.4375 - dense_2_acc_5: 0.6667 - dense_2_acc_6: 0.7292 - dense_2_acc_7: 0.8958 - dense_2_acc_8: 0.9375 - dense_2_acc_9: 0.9583 - dense_2_acc_10: 0.9792 - dense_2_acc_11: 0.9792 - dense_2_acc_12: 1.00 - ETA: 1s - loss: 14.8449 - dense_2_loss_1: 2.7851 - dense_2_loss_2: 3.1949 - dense_2_loss_3: 3.3570 - dense_2_loss_4: 2.4335 - dense_2_loss_5: 1.3493 - dense_2_loss_6: 0.8946 - dense_2_loss_7: 0.4024 - dense_2_loss_8: 0.1470 - dense_2_loss_9: 0.1218 - dense_2_loss_10: 0.0749 - dense_2_loss_11: 0.0738 - dense_2_loss_12: 0.0105 - dense_2_acc_1: 0.3125 - dense_2_acc_2: 0.3281 - dense_2_acc_3: 0.2500 - dense_2_acc_4: 0.4219 - dense_2_acc_5: 0.6875 - dense_2_acc_6: 0.7812 - dense_2_acc_7: 0.9219 - dense_2_acc_8: 0.9531 - dense_2_acc_9: 0.9688 - dense_2_acc_10: 0.9844 - dense_2_acc_11: 0.9844 - dense_2_acc_12: 1.00 - ETA: 1s - loss: 14.8091 - dense_2_loss_1: 2.9001 - dense_2_loss_2: 3.2183 - dense_2_loss_3: 3.4188 - dense_2_loss_4: 2.3343 - dense_2_loss_5: 1.3601 - dense_2_loss_6: 0.7997 - dense_2_loss_7: 0.3786 - dense_2_loss_8: 0.1673 - dense_2_loss_9: 0.1009 - dense_2_loss_10: 0.0616 - dense_2_loss_11: 0.0602 - dense_2_loss_12: 0.0093 - dense_2_acc_1: 0.3000 - dense_2_acc_2: 0.3250 - dense_2_acc_3: 0.2375 - dense_2_acc_4: 0.4500 - dense_2_acc_5: 0.6875 - dense_2_acc_6: 0.8125 - dense_2_acc_7: 0.9250 - dense_2_acc_8: 0.9500 - dense_2_acc_9: 0.9750 - dense_2_acc_10: 0.9875 - dense_2_acc_11: 0.9875 - dense_2_acc_12: 1.00 - ETA: 1s - loss: 14.8630 - dense_2_loss_1: 2.8710 - dense_2_loss_2: 3.3344 - dense_2_loss_3: 3.3024 - dense_2_loss_4: 2.3576 - dense_2_loss_5: 1.3593 - dense_2_loss_6: 0.8079 - dense_2_loss_7: 0.3526 - dense_2_loss_8: 0.1923 - dense_2_loss_9: 0.1203 - dense_2_loss_10: 0.0901 - dense_2_loss_11: 0.0603 - dense_2_loss_12: 0.0148 - dense_2_acc_1: 0.2917 - dense_2_acc_2: 0.2917 - dense_2_acc_3: 0.2500 - dense_2_acc_4: 0.4375 - dense_2_acc_5: 0.6771 - dense_2_acc_6: 0.8125 - dense_2_acc_7: 0.9375 - dense_2_acc_8: 0.9479 - dense_2_acc_9: 0.9688 - dense_2_acc_10: 0.9792 - dense_2_acc_11: 0.9896 - dense_2_acc_12: 1.00 - ETA: 1s - loss: 15.2275 - dense_2_loss_1: 2.9156 - dense_2_loss_2: 3.4155 - dense_2_loss_3: 3.2340 - dense_2_loss_4: 2.5048 - dense_2_loss_5: 1.3995 - dense_2_loss_6: 0.8524 - dense_2_loss_7: 0.4216 - dense_2_loss_8: 0.1985 - dense_2_loss_9: 0.1397 - dense_2_loss_10: 0.0793 - dense_2_loss_11: 0.0530 - dense_2_loss_12: 0.0138 - dense_2_acc_1: 0.2857 - dense_2_acc_2: 0.2679 - dense_2_acc_3: 0.2768 - dense_2_acc_4: 0.4107 - dense_2_acc_5: 0.6607 - dense_2_acc_6: 0.8036 - dense_2_acc_7: 0.9196 - dense_2_acc_8: 0.9464 - dense_2_acc_9: 0.9643 - dense_2_acc_10: 0.9821 - dense_2_acc_11: 0.9911 - dense_2_acc_12: 1.00 - ETA: 1s - loss: 15.4406 - dense_2_loss_1: 2.8358 - dense_2_loss_2: 3.4478 - dense_2_loss_3: 3.2120 - dense_2_loss_4: 2.4472 - dense_2_loss_5: 1.5230 - dense_2_loss_6: 0.9450 - dense_2_loss_7: 0.4325 - dense_2_loss_8: 0.2309 - dense_2_loss_9: 0.1797 - dense_2_loss_10: 0.1074 - dense_2_loss_11: 0.0597 - dense_2_loss_12: 0.0196 - dense_2_acc_1: 0.3125 - dense_2_acc_2: 0.2656 - dense_2_acc_3: 0.2812 - dense_2_acc_4: 0.4375 - dense_2_acc_5: 0.6406 - dense_2_acc_6: 0.7812 - dense_2_acc_7: 0.9141 - dense_2_acc_8: 0.9375 - dense_2_acc_9: 0.9531 - dense_2_acc_10: 0.9766 - dense_2_acc_11: 0.9922 - dense_2_acc_12: 1.00 - ETA: 0s - loss: 15.6793 - dense_2_loss_1: 2.7485 - dense_2_loss_2: 3.4571 - dense_2_loss_3: 3.2508 - dense_2_loss_4: 2.4992 - dense_2_loss_5: 1.5905 - dense_2_loss_6: 1.0206 - dense_2_loss_7: 0.4874 - dense_2_loss_8: 0.2357 - dense_2_loss_9: 0.1866 - dense_2_loss_10: 0.1290 - dense_2_loss_11: 0.0552 - dense_2_loss_12: 0.0186 - dense_2_acc_1: 0.3333 - dense_2_acc_2: 0.2708 - dense_2_acc_3: 0.2639 - dense_2_acc_4: 0.4375 - dense_2_acc_5: 0.6250 - dense_2_acc_6: 0.7569 - dense_2_acc_7: 0.9028 - dense_2_acc_8: 0.9375 - dense_2_acc_9: 0.9514 - dense_2_acc_10: 0.9722 - dense_2_acc_11: 0.9931 - dense_2_acc_12: 1.00 - ETA: 0s - loss: 15.4915 - dense_2_loss_1: 2.6915 - dense_2_loss_2: 3.4281 - dense_2_loss_3: 3.2450 - dense_2_loss_4: 2.5322 - dense_2_loss_5: 1.5542 - dense_2_loss_6: 1.0044 - dense_2_loss_7: 0.4703 - dense_2_loss_8: 0.2134 - dense_2_loss_9: 0.1685 - dense_2_loss_10: 0.1166 - dense_2_loss_11: 0.0502 - dense_2_loss_12: 0.0172 - dense_2_acc_1: 0.3500 - dense_2_acc_2: 0.2875 - dense_2_acc_3: 0.2687 - dense_2_acc_4: 0.4313 - dense_2_acc_5: 0.6375 - dense_2_acc_6: 0.7625 - dense_2_acc_7: 0.9062 - dense_2_acc_8: 0.9437 - dense_2_acc_9: 0.9563 - dense_2_acc_10: 0.9750 - dense_2_acc_11: 0.9938 - dense_2_acc_12: 1.00 - ETA: 0s - loss: 15.5693 - dense_2_loss_1: 2.6978 - dense_2_loss_2: 3.4864 - dense_2_loss_3: 3.2275 - dense_2_loss_4: 2.6263 - dense_2_loss_5: 1.5571 - dense_2_loss_6: 0.9677 - dense_2_loss_7: 0.4656 - dense_2_loss_8: 0.2175 - dense_2_loss_9: 0.1550 - dense_2_loss_10: 0.1066 - dense_2_loss_11: 0.0460 - dense_2_loss_12: 0.0160 - dense_2_acc_1: 0.3409 - dense_2_acc_2: 0.2727 - dense_2_acc_3: 0.2670 - dense_2_acc_4: 0.4091 - dense_2_acc_5: 0.6420 - dense_2_acc_6: 0.7727 - dense_2_acc_7: 0.9091 - dense_2_acc_8: 0.9432 - dense_2_acc_9: 0.9602 - dense_2_acc_10: 0.9773 - dense_2_acc_11: 0.9943 - dense_2_acc_12: 1.00 - ETA: 0s - loss: 15.7479 - dense_2_loss_1: 2.6946 - dense_2_loss_2: 3.4795 - dense_2_loss_3: 3.2489 - dense_2_loss_4: 2.6815 - dense_2_loss_5: 1.6304 - dense_2_loss_6: 0.9636 - dense_2_loss_7: 0.4769 - dense_2_loss_8: 0.2226 - dense_2_loss_9: 0.1652 - dense_2_loss_10: 0.1230 - dense_2_loss_11: 0.0460 - dense_2_loss_12: 0.0158 - dense_2_acc_1: 0.3281 - dense_2_acc_2: 0.2865 - dense_2_acc_3: 0.2604 - dense_2_acc_4: 0.3958 - dense_2_acc_5: 0.6250 - dense_2_acc_6: 0.7760 - dense_2_acc_7: 0.9062 - dense_2_acc_8: 0.9427 - dense_2_acc_9: 0.9531 - dense_2_acc_10: 0.9740 - dense_2_acc_11: 0.9948 - dense_2_acc_12: 1.00 - 3s 15ms/step - loss: 15.6696 - dense_2_loss_1: 2.7046 - dense_2_loss_2: 3.4577 - dense_2_loss_3: 3.2428 - dense_2_loss_4: 2.6728 - dense_2_loss_5: 1.6117 - dense_2_loss_6: 0.9564 - dense_2_loss_7: 0.4690 - dense_2_loss_8: 0.2169 - dense_2_loss_9: 0.1593 - dense_2_loss_10: 0.1185 - dense_2_loss_11: 0.0445 - dense_2_loss_12: 0.0154 - dense_2_acc_1: 0.3300 - dense_2_acc_2: 0.2900 - dense_2_acc_3: 0.2600 - dense_2_acc_4: 0.3950 - dense_2_acc_5: 0.6300 - dense_2_acc_6: 0.7800 - dense_2_acc_7: 0.9100 - dense_2_acc_8: 0.9450 - dense_2_acc_9: 0.9550 - dense_2_acc_10: 0.9750 - dense_2_acc_11: 0.9950 - dense_2_acc_12: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efdd39b588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm_model.fit([en_oh, s0, c0], outputs, epochs = 1, batch_size= 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Your Own Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: i love you\n",
      "outputs: je t'aime . . . . . . . . . .\n",
      "source: i hate you\n",
      "outputs: je veux . . . . . . . . . .\n",
      "source: you are my love\n",
      "outputs: tu es mon . . . . . . . . .\n",
      "source: steve is genius\n",
      "outputs: elle est est . . . . . . . . .\n",
      "source: ha ha ha wow\n",
      "outputs: ha ha ha . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['i love you', 'i hate you', 'you are my love', 'steve is genius', 'ha ha ha wow']\n",
    "for examples in EXAMPLES:\n",
    "    source = convert_example_to_indices(examples, english_vocab, Ten)\n",
    "    source = convert_to_one_hot(source, C= len(english_vocab)).reshape(1, Ten, len(english_vocab))\n",
    "    predict = nm_model.predict([source, s0, c0])\n",
    "    predict = np.argmax(predict, axis = -1)\n",
    "    output  = [inv_french_vocab[int(i)] for i in predict]\n",
    "    \n",
    "    print(\"source:\", examples)\n",
    "    print(\"outputs:\", ' '.join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
